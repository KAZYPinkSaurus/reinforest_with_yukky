{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    plt.figure(figsize=(frames[0].shape[1]/72,frames[0].shape[0]/72),dpi=72)\n",
    "    patch=plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "    anim=animation.FuncAnimation(plt.gcf(),animate,frames=len(frames),interval=50)\n",
    "    anim.save('movie_cartpole.mp4')\n",
    "    display(display_animation(anim,default_mode='loop'))\n",
    "\n",
    "frames=[]\n",
    "env=gym.make('CartPole-v0')\n",
    "observation=env.reset()\n",
    "for step in range(0,200):\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    action=np.random.choice(2)\n",
    "    observation,reward,done,info=env.step(action)\n",
    "\n",
    "display_frames_as_gif(frames)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "ENV='CartPole-v0'\n",
    "NUM_DIZITIZED=6\n",
    "env=gym.make(ENV)\n",
    "observation=env.reset()\n",
    "\n",
    "def bins(clip_min,clip_max,num):\n",
    "    return np.linspace(clip_min,clip_max,num+1)[1:-1]\n",
    "\n",
    "def digitize_state(observation):\n",
    "    cart_pos,cart_v,pole_angle,pole_v=observation\n",
    "    digitized=[\n",
    "        np.digitize(cart_pos,bins=bins(-2.4,2.4,NUM_DIZITIZED)),\n",
    "        np.digitize(cart_pos,bins=bins(-3.0,3.0,NUM_DIZITIZED)),\n",
    "        np.digitize(cart_pos,bins=bins(-0.5,0.5,NUM_DIZITIZED)),\n",
    "        np.digitize(cart_pos,bins=bins(-2.0,2.0,NUM_DIZITIZED))]\n",
    "    return sum([x*(NUM_DIZITIZED**i) for i,x in enumerate(digitized)])\n",
    "\n",
    "digitize_state(observation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ENV='CartPole-v0'\n",
    "NUM_DIZITIZED=6\n",
    "GAMMA=0.99\n",
    "ETA=0.5\n",
    "MAX_STEPS=200\n",
    "NUM_EPISODES=1000\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,num_states,num_actions):\n",
    "        self.brain=Brain(num_states,num_actions)\n",
    "    \n",
    "    def update_Q_function(self,observation,action,reward,observation_next):\n",
    "        self.brain.update_Q_table(observation,action,reward,observation_next)\n",
    "        \n",
    "    def get_action(self,observation,step):\n",
    "        action=self.brain.decide_action(observation,step)\n",
    "        return action\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self,num_states,num_actions):\n",
    "        self.num_actions=num_actions\n",
    "        self.q_table=np.random.uniform(low=0,high=1,size=(NUM_DIZITIZED**num_states,num_actions))\n",
    "    \n",
    "    def bins(self,clip_min,clip_max,num):\n",
    "        return np.linspace(clip_min,clip_max,num+1)[1:-1]\n",
    "        \n",
    "    def digitize_state(self,observation):\n",
    "        cart_pos,cart_v,pole_angle,pole_v=observation\n",
    "        digitized=[\n",
    "            np.digitize(cart_pos,bins=bins(-2.4,2.4,NUM_DIZITIZED)),\n",
    "            np.digitize(cart_pos,bins=bins(-3.0,3.0,NUM_DIZITIZED)),\n",
    "            np.digitize(cart_pos,bins=bins(-0.5,0.5,NUM_DIZITIZED)),\n",
    "            np.digitize(cart_pos,bins=bins(-2.0,2.0,NUM_DIZITIZED))]\n",
    "        return sum([x*(NUM_DIZITIZED**i) for i,x in enumerate(digitized)])\n",
    "    \n",
    "    def update_Q_table(self,observation,action,reward,observation_next):\n",
    "        state=self.digitize_state(observation)\n",
    "        state_next=self.digitize_state(observation_next)\n",
    "        Max_Q_next=max(self.q_table[state_next][:])\n",
    "        self.q_table[state,action]=self.q_table[state,action]+ETA*(reward+GAMMA*Max_Q_next-self.q_table[state,action])\n",
    "    \n",
    "    def decide_action(self,observation,episode):\n",
    "        state=self.digitize_state(observation)\n",
    "        epsilon=0.5*(1/(episode+1))\n",
    "        if epsilon<=np.random.uniform(0,1):\n",
    "            action=np.argmax(self.q_table[state][:])\n",
    "        else:\n",
    "            action=np.random.choice(self.num_actions)\n",
    "        return action\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env=gym.make(ENV)\n",
    "        num_states=self.env.observation_space.shape[0]\n",
    "        num_actions=self.env.action_space.n\n",
    "        self.agent=Agent(num_states,num_actions)\n",
    "        \n",
    "    def run(self):\n",
    "        complete_episodes=0\n",
    "        is_episode_final=False\n",
    "        frames=[]\n",
    "        for episode in range(NUM_EPISODES):\n",
    "            observation=self.env.reset()\n",
    "            for step in range(MAX_STEPS):\n",
    "                if is_episode_final is True:\n",
    "                    frames.append(self.env.render(mode='rgb_array')) \n",
    "                action=self.agent.get_action(observation,episode)\n",
    "                observation_next,_,done,_=self.env.step(action)\n",
    "                if done:\n",
    "                    if step<195:\n",
    "                        reward=-1\n",
    "                        complete_episodes=0\n",
    "                    else:\n",
    "                        reward=1\n",
    "                        complete_episodes+=1\n",
    "                else:\n",
    "                    reward=0\n",
    "                    self.agent.update_Q_function(observation,action,reward,observation_next)\n",
    "                    observation=observation_next\n",
    "                    if done:\n",
    "                        print('{0} Episode:Finished after {1} time steps'.format(episode,step+1))\n",
    "                if is_episode_final is True:\n",
    "                    display_frames_as_gif(frames)\n",
    "                    break\n",
    "                if complete_episodes>=10:\n",
    "                    print('10回連続成功')\n",
    "                    is_episode_final=True\n",
    "\n",
    "cartpole_env=Environment()\n",
    "cartpole_env.run()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
